{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概述\n",
    "## 决策树是如何工作的\n",
    "决策树（Decision Tree）是一种非参数的有监督学习方法，它能够从一系列有特征和标签的数据中总结出决策规\n",
    "则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。决策树算法容易理解，适用各种数据，在解决各\n",
    "种问题时都有良好表现，尤其是以树模型为核心的各种集成算法，在各个行业和领域都有广泛的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建决策数的两个阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. 训练阶段  \n",
    "从给定的训练数据集DB，构造出一颗决策树  \n",
    "`class = DecisionTree(DB)`   \n",
    "2. 分类阶段  \n",
    "从根开始，按照决策树的分类属性逐层往下划分，知道叶节点，获得概念(决策、分类)  \n",
    "`y = DecisionTree(x)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建决策树\n",
    "原则上讲，任意一个数据集上的所有特征都可以被拿来分\n",
    "枝，特征上的任意节点又可以自由组合，所以一个数据集上可以发展出非常非常多棵决策树，其数量可达指数级。在\n",
    "这些树中，总有那么一棵树比其他的树分类效力都好，那样的树叫做”全局最优树“。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**关键概念：全局最优、局部最优**|\n",
    "|  :------------------------ |  \n",
    "|全局最优：经过组合形成的，整体来说分类效果最好的模型|\n",
    "|局部最优：每一次分枝的时候都向着更好的分类效果分枝，但无法确认如此生成的树在全局上是否是最优的|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要在这么多棵决策树中去一次性找到分类效果最佳的那一棵是不可能的，如果通过排列组合来进行筛选，计算量过于\n",
    "大而且低效，因此我们不会这样做。相对的，机器学习研究者们开发了一些有效的算法，能够在合理的时间内构造出\n",
    "具有一定准确率的**次最优**决策树。这些算法基本都执行”**贪心策略**“，即通过局部的最优来达到我们相信是最接近全局\n",
    "最优的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|关键概念：贪心算法|\n",
    "| :---------|\n",
    "|通过实现局部最优来达到接近全局最优结果的算法，所有的树模型都是这样的算法。|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法的核心解决问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 如何从数据表中找出最佳点和最佳分支\n",
    "2. 如何让决策树停止生长，防止过拟合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择根节点(信息增益最大的当成根节点)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树-熵\n",
    ">熵：表示数据内部混乱程度。熵越大越混乱。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关符合表示：  \n",
    "$H(X), H(Y)：表示当前事情X,Y发生的不确定性。$  \n",
    "$P(X)，P(Y)：表示当前事情发生的概率。$  \n",
    "$P(X)越大 --> H(X)越小。(可以这么理解，一件事情发生的概率越大说明它的不确定性越小。)$  \n",
    "$P(X)越小 --> H(X)越大。(可以这么理解，一件事情发生的概率越小说明它的不确定性越大。)$  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设：$A = \\{1,2,3,4,1,2,1\\}$  数字表示A里面的类别  \n",
    "$B = \\{1,1,1,1,1,2\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于A的类别很多，杂乱无章，我们可以说A的熵值很大，不纯度高。   \n",
    "由于B的类别很少(1或者2), B相对于A而言，我们可以说B的熵值很小，不纯度低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|重要概念：不纯度|  \n",
    "| :----- |\n",
    "|决策树的每个叶子节点中都会包含一组数据，在这组数据中，如果有某一类标签占有较大的比例，我们就说叶子\n",
    "节点“纯”，分枝分得好。某一类标签占的比例越大，叶子就越纯，不纯度就越低，分枝就越好。\n",
    "如果没有哪一类标签的比例很大，各类标签都相对平均，则说叶子节点”不纯“，分枝不好，不纯度高|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$熵= - \\displaystyle\\sum_{i=1}^{n}P_iln(P_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/shang.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理解：当P比较小时，对应的lnP会比较大，所以能得到较大是熵，说明当前集合的混乱程度比较大。  \n",
    "当P比较大时，对应的lnP会比较小，所以得到的熵值比较小，说明当前集合的混乱程度比较小。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini(基尼系数)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Gini系数 = Gini(p) = \\displaystyle \\sum_{k=1}^{K}p_k(1-p_k) = 1 - \\displaystyle \\sum_{k=1}^{K}p_k^2  \n",
    "\\text{(p为某一类别发生的概率)}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造决策树的基本想法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">构造决策树的基本想法是随着树深度的增加，节点的熵**迅速下降**。熵下降的速度越快越好，这样我们就可以构造一颗**最矮**的决策树。(能用少量有用的信息解决事情越好)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据前面的特征(outlook, temperature, humidty, windy)，判断是否出去打篮球(play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/example1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/decision1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.计算原始数据的熵值\n",
    "在没有给定任何天气信息时，根据历史数据，我们能知道一天打球的概率是9/14， 不打的概率是5/14。此时的熵为：  \n",
    "$-\\dfrac{9}{14}log_2\\dfrac{9}{14} - \\dfrac{5}{14}log_2\\dfrac{5}{14} = 0.940$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 计算候选节点的熵值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们计算当已知变量outlook的值时，信息熵为多少？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/decision2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对其加权求和"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/decision3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">信息增益表示：熵值发生的变化。  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/decision4.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3：：信息增益算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益的缺点：一些特征里面存在的属性很多，但是每个属性对应的的样本很少，这样会造成很大的信息增益。  \n",
    "示例： 我们为每一个样本增加一个ID(ID与结论无关，只是数据的一个标志)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/decision5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们先考虑把ID当成根节点：  \n",
    "那么ID对应的信息熵为：  \n",
    "$\\dfrac{1}{14}\\times(-1\\times log_2(1)) + \\cdots + \\dfrac{1}{14}\\times(-1\\times log_2(1)) = 0$  \n",
    "熵值为0，信息增益最大，我们知道ID只是对样本的一个编号，不会对结论造成影响。但是，通过计算得到的结果会让我们错误地把ID当成根节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5(信息增益率)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对应每个属性除以它自身的信息增益再求累加和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评价函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C(T) = \\displaystyle \\sum_{t∈leaf}N_tH(t)$  （类似于损失函数，越小越好）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连续值的分界选择(如，年龄)\n",
    "![](images/decision6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 剪枝操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要剪枝？  \n",
    "如果不剪枝的话，很容易导致过拟合。   \n",
    "如果决策树过于庞大，它包含的信息可能在训练集上很不错，但在测试集上很容易导致过拟合了，把一些噪音也涵盖进去了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">预剪枝：在构建决策树过程中，能提前停止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 设置最大深度\n",
    "2. 设置最小节点\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 后剪枝\n",
    ">后剪枝：构建完决策树后，才开始剪枝操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 修改损失函数\n",
    "![](images/decision7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机森林(相当于构造多颗决策树)\n",
    "## 相关概念\n",
    "### 采样方式\n",
    "1. Bootstraping：有放回采样  \n",
    "2. Bagging：有放回采样n个样本建立分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机\n",
    "### 双重随机性\n",
    "1. 数据选择的随机性\n",
    "2. 特征随机性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/decision8.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
