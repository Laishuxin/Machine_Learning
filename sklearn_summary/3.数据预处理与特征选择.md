# 机器学习流程

1. 数据获取
2. 数据预处理
3. 特征工程
4. 建模
5. 上线

## sklearn中数据预处理模块

+ preprocessing
+ impute
+ feature_selection
+ decomposition

# 数据预处理Preprocessing&Impute

## 数据无量纲化(去单位化)

### 归一化

+ 最大最小值归一化(preprocessing.MinMaxScaler)

+ 指定缩放区间的归一化(preprocessing.MinMaxScaler(feature_range=(min, max)))

+ 均值归一化
+ 非线性归一化

### 标准化

preprocessing.StandardScaler



### 为什么要归一化/标准化

1. 某些模型需要
2. 加快模型运行速度

### 如何选择归一化/标准化

1. 大多数算法用标准化(因为MinMaxScaler对异常值非常敏感)

2. 在不涉及距离度量、梯度、协方差计算以及需要将数据压缩到指定区间时(数字图像处理)，用MinMaxScaler

## 处理缺失值(impute.SimpleImputer)

SimpleImputer(missing_values, strategy, fill_value, copy)

1. 均值填充

SimpleImputer(missing_values=np.nan, strategy='mean')

2. 填充0

SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)

3. 填充中位数

SimpleImputer(missing_values=np.nan, strategy='median')

4. 填充众数（支持字符串处理）

SimpleImputer(missing_values=np.nan, strategy='most_frequent')

## 处理分类类型特征(编码与哑变量)

1. preprocessing.LabelEncoder处理分类型标签

label.classes_

2. preprocessing.OrdinalEncoder处理分类型特征

orc = OrdinalEncoder().fit(X)

orc.categories_ 查看分类

3. preprocessing.OneHotEncoder(独热变量，哑变量)处理名义变量

+ 名义变量
+ 有序变量
+ 有矩变量

ohe = OneHotEncoder().fit(X)

ohe.get_feature_names() 返回哑变量的标签名

OneHotEncoder.fit(X).toarray()

4. preprocessing.LabelBinarize处理哑变量标签

5. preprocessing.Binarizer二值化

> 二值化：大于阈值为1，小于阈值为0

Binarizer(threshold).fit_transform(X)



6. KBinsDiscretizer(分段化，离散化)

> 这是将连续型变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。总共包含三个重要参数：
>
> n_bins  
>
> encode: 'onehot', 'ordinal', 'onehot-dense'
>
> strategy: quantile, uniform, kmeans

# 特征工程

特征工程的基本思想：  
$$全部特征\Rightarrow最佳特征子集\Rightarrow算法\Rightarrow模型评估$$

## 特征选择

### 过滤法(**主要对象是：需要遍历特征或升维的算法**们，而**过滤法的主要目的**是：在维持算法表现的前提下，帮助算法们降低计算成本)

1. 方差过滤(feature_selection.VarianceThreshold(threshold=0))

2. 卡方过滤(feature_selection.chi2, feature_selection.SelectKBest)

> 针对离散型标签，分类问题的相关性过滤。默认原假设为相互独立

+ SelectKBest(chi2, k=k)

+ p值过滤法

chi_values, chi_pvalues = chi2(X, y)

p = 0.05 or 0.01

k = chi_values.shape[0] - (chi_p_values>p).sum()

X_chi = SelectKBest(chi2, k=k).fit_transform(X, y )

2. F过滤(feature_selection.f_classif，feature_selection.SelectKBest)可以分类也可以回归  

    > F检验的本质是检验两组数据之间是否由线性关系。默认原假设为两组数据之间没有线性关系。 使用F检验时，先将数据标准化会比较好。  

    F_values, F_pvalues = f_classif(X, y)

    p = 0.05 or 0.01

    k = F_values.shape[1] - (F_pvalues>p).sum()

    X_F = SelectKBest(f_classif, k=k).fit_transform(X, y)

    3. 互信息法过滤(feature_selection.mutual_info_classif)可以回归可以分类

    > 互信息法是用来捕捉每个特征与标准之间的任意关系(包括线性关系与非线性关系)。
    >
    > 互信息法不返回p值等统计量，它返回每个特征与目标之间的互信息量的估计。估计量在(0, 1]表示两组数据之间存在关系。$[-\infty,0]$表示两组数据之间不存在关系，0为相互独立。  

    p_values = mutual_info_classif(X, y)

    k = X.shape[1] - (p_values<=0).sum()

    X_mic = SelectKBest(mutual_info_classif, k=k).fit_transform(X, y)

    ### Embedded嵌入法(feature_selection.SelectFromModel)

    > 嵌入法是让算法自己选用哪些特征的方法，是特征选择与算法训练同时进行。它会根据算法返回的特征权重(coef_, feature_importances_)等属性对特征进行评估，选择合适的特征。（精确到每一个模型，运行速度受模型复杂度影响）

    from sklearn.ensemble import RandomForestClassifier as RFC

    threshold=0.001等

    SelectFromModel(RFC(), threshold=threshold).fit_transform(X, y)

    ## wrapper包装法(feature_selection.RFE)

    > 包装法也是一种特性选择与模型训练同时进行的算法。  
    >
    > 包装法会在初始特征集上训练评估器，通过coef_或feature_importances_等属性获得每个特征的重要性。  
    >
    > 通过不断递归地取前m个表现优秀的特征(剔除那些表现差的特征，下一个递归会在上一次没有被选到的特征进行模型训练)。  
    >
    > 这是一种开销非常大的特征选择算法。  
    >
    > 包装法最典型的目标函数是递归特征消元法(Recursive feature elimination, RFE)

    from sklearn.ensemble import RandomForestClassifier  as RFC

    X_RFE = RFE(RFC(), n_feature_select=, step=).fit(X, y)

X_RFE.support_

X_RFE.ranking_

# 小结

在特征过滤算法中，  
过滤法的速度最快，但效果最粗糙。  
嵌入法和包装法更精确，具体到每个模型，但开销也由模型的复杂度增加而增加。  
优先使用过滤法，简单地过滤一些没有用的特征，再进一步考虑嵌入法和包装法。  
当数据量很大的时候，优先使用方差过滤和互信息法调整，再上其他特
征选择方法。使用逻辑回归时，优先使用嵌入法。使用支持向量机时，优先使用包装法。迷茫的时候，从过滤法走
起，看具体数据具体分析。