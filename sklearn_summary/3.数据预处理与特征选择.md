# 机器学习流程

1. 数据获取
2. 数据预处理
3. 特征工程
4. 建模
5. 上线

## sklearn中数据预处理模块

+ preprocessing
+ impute
+ feature_selection
+ decomposition

# 数据预处理Preprocessing&Impute

## 数据无量纲化(去单位化)

### 归一化

+ 最大最小值归一化(preprocessing.MinMaxScaler)

+ 指定缩放区间的归一化(preprocessing.MinMaxScaler(feature_range=(min, max)))

+ 均值归一化
+ 非线性归一化

### 标准化

preprocessing.StandardScaler



### 为什么要归一化/标准化

1. 某些模型需要
2. 加快模型运行速度

### 如何选择归一化/标准化

1. 大多数算法用标准化(因为MinMaxScaler对异常值非常敏感)

2. 在不涉及距离度量、梯度、协方差计算以及需要将数据压缩到指定区间时(数字图像处理)，用MinMaxScaler

## 处理缺失值(impute.SimpleImputer)

SimpleImputer(missing_values, strategy, fill_value, copy)

1. 均值填充

SimpleImputer(missing_values=np.nan, strategy='mean')

2. 填充0

SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)

3. 填充中位数

SimpleImputer(missing_values=np.nan, strategy='median')

4. 填充众数（支持字符串处理）

SimpleImputer(missing_values=np.nan, strategy='most_frequent')

## 处理分类类型特征(编码与哑变量)

1. preprocessing.LabelEncoder处理分类型标签

label.classes_

2. preprocessing.OrdinalEncoder处理分类型特征

orc = OrdinalEncoder().fit(X)

orc.categories_ 查看分类

3. preprocessing.OneHotEncoder(独热变量，哑变量)处理名义变量

+ 名义变量
+ 有序变量
+ 有矩变量

ohe = OneHotEncoder().fit(X)

ohe.get_feature_names() 返回哑变量的标签名

OneHotEncoder.fit(X).toarray()

4. preprocessing.LabelBinarize处理哑变量标签

5. preprocessing.Binarizer二值化

> 二值化：大于阈值为1，小于阈值为0

Binarizer(threshold).fit_transform(X)



6. KBinsDiscretizer(分段化，离散化)

> 这是将连续型变量划分为分类变量的类，能够将连续型变量排序后按顺序分箱后编码。总共包含三个重要参数：
>
> n_bins  
>
> encode: 'onehot', 'ordinal', 'onehot-dense'
>
> strategy: quantile, uniform, kmeans

# 特征工程

特征工程的基本思想：  
$$全部特征\Rightarrow最佳特征子集\Rightarrow算法\Rightarrow模型评估$$

## 特征选择

### 过滤法(**主要对象是：需要遍历特征或升维的算法**们，而**过滤法的主要目的**是：在维持算法表现的前提下，帮助算法们降低计算成本)

1. 方差过滤(feature_selection.VarianceThreshold(threshold=0))

2. 卡方过滤(feature_selection.chi2, feature_selection.SelectKBest)

> 针对离散型标签，分类问题的相关性过滤。默认原假设为相互独立

+ SelectKBest(chi2, k=k)

+ p值过滤法

chi_values, chi_pvalues = chi2(X, y)

p = 0.05 or 0.01

k = chi_values.shape[0] - (chi_p_values>p).sum()

SelectKBest(chi2, k=k)

2. F过滤(feature_selection.f_classif)

     